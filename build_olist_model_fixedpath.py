#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build Olist relational model (per Kaggle's "Brazilian E-Commerce by Olist")
and create a denormalized fact table following the provided schema image.

- Reads either CSV or Excel files from DATA_DIR
- Performs the relationships:
    orders <-> order_items (order_id)
    orders <-> order_payments (order_id)
    orders <-> order_reviews (order_id)
    orders <-> customers (customer_id)
    order_items <-> products (product_id)
    order_items <-> sellers (seller_id)
    customers <-> geolocation (zip_code_prefix)
    sellers <-> geolocation (zip_code_prefix)
    products <-> product_category_name_translation (product_category_name)
- Writes outputs to the OUTPUT_DIR as CSV and Parquet (if pyarrow installed)

Author: Generated by ChatGPT
"""

import os
import sys
from typing import Optional, Dict, Any, List
import pandas as pd

# --------------- Configuration ----------------
# Change this to your folder containing the Olist files.
# By default it tries the current working directory.
DATA_DIR = "/Users/leonardo/Documents/PyCharm/Github/ProjetoExtracaoEco/extracao/base_de_dados"

# Where to save the outputs
OUTPUT_DIR = os.environ.get("OLIST_OUTPUT_DIR", os.path.join(DATA_DIR, "outputs"))
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --------------- Helpers ----------------
def _read_any(path: str, **kwargs) -> pd.DataFrame:
    """
    Read a CSV or Excel file into a DataFrame.
    - If path endswith .csv -> uses read_csv (tries to infer sep)
    - If path endswith .xlsx or .xls -> uses read_excel
    kwargs are passed to pandas reader.
    """
    lower = path.lower()
    if lower.endswith(".csv"):
        # sep inference: default to comma, but handle semicolons gracefully
        try:
            return pd.read_csv(path, dtype=str, low_memory=False, **kwargs)
        except Exception:
            return pd.read_csv(path, dtype=str, sep=";", low_memory=False, **kwargs)
    elif lower.endswith(".xlsx") or lower.endswith(".xls"):
        return pd.read_excel(path, dtype=str, engine="openpyxl" if lower.endswith(".xlsx") else None, **kwargs)
    else:
        raise ValueError(f"Unsupported file type: {path}")

def _exists_any(data_dir: str, base: str) -> Optional[str]:
    """
    Find a file in data_dir with given base name, trying .csv and Excel variants.
    Returns the path or None.
    """
    candidates = [
        f"{base}.csv",
        f"{base}.CSV",
        f"{base}.xlsx",
        f"{base}.xls",
    ]
    for c in candidates:
        p = os.path.join(data_dir, c)
        if os.path.exists(p):
            return p
    return None

def load_tables(data_dir: str) -> Dict[str, pd.DataFrame]:
    """
    Load all required Olist tables if present.
    Returns a dict of DataFrames keyed by short name.
    """
    file_map = {
        "orders": "olist_orders_dataset",
        "order_items": "olist_order_items_dataset",
        "order_payments": "olist_order_payments_dataset",
        "order_reviews": "olist_order_reviews_dataset",
        "customers": "olist_customers_dataset",
        "products": "olist_products_dataset",
        "sellers": "olist_sellers_dataset",
        "geolocation": "olist_geolocation_dataset",
        "category_translation": "product_category_name_translation",
    }

    dfs: Dict[str, pd.DataFrame] = {}
    for key, base in file_map.items():
        path = _exists_any(data_dir, base)
        if path is None:
            print(f"[WARN] Not found: {base} (expected as CSV/XLSX) in {data_dir}")
            continue
        df = _read_any(path)
        # Standardize to lowercase columns
        df.columns = [c.strip().lower() for c in df.columns]
        dfs[key] = df
        print(f"[OK] Loaded {key:<20s} -> {df.shape} from {os.path.basename(path)}")
    return dfs

def build_denorm(dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    Create a denormalized order lines fact table following the schema.
    Returns the denormalized DataFrame.
    """
    required = ["orders", "order_items", "customers", "sellers", "products"]
    for r in required:
        if r not in dfs:
            raise RuntimeError(f"Missing required table: {r}")

    orders = dfs["orders"].copy()
    items = dfs["order_items"].copy()
    customers = dfs["customers"].copy()
    sellers = dfs["sellers"].copy()
    products = dfs["products"].copy()

    # Convert timestamps where present
    for col in ["order_purchase_timestamp", "order_approved_at", "order_delivered_carrier_date",
                "order_delivered_customer_date", "order_estimated_delivery_date"]:
        if col in orders.columns:
            orders[col] = pd.to_datetime(orders[col], errors="coerce")

    # ---- Aggregate payments per order ----
    pay_cols_keep = ["order_id", "payment_sequential", "payment_type", "payment_installments", "payment_value"]
    if "order_payments" in dfs:
        pays = dfs["order_payments"][list(set(dfs["order_payments"].columns) & set(pay_cols_keep))].copy()
        # aggregate payment_value per type and the total per order
        pays["payment_value"] = pd.to_numeric(pays.get("payment_value", pd.Series(dtype="float")), errors="coerce")
        agg_total = pays.groupby("order_id", as_index=False).agg(total_payment_value=("payment_value", "sum"),
                                                                 n_payments=("payment_sequential", "count"))
    else:
        agg_total = pd.DataFrame(columns=["order_id", "total_payment_value", "n_payments"])

    # ---- Aggregate reviews per order ----
    if "order_reviews" in dfs:
        rev = dfs["order_reviews"].copy()
        for c in ["review_score", "review_comment_title", "review_comment_message"]:
            if c not in rev.columns:
                # gracefully skip if columns don't exist
                pass
        rev["review_score"] = pd.to_numeric(rev.get("review_score", pd.Series(dtype="float")), errors="coerce")
        agg_rev = rev.groupby("order_id", as_index=False).agg(
            review_score_mean=("review_score", "mean"),
            review_score_median=("review_score", "median"),
            n_reviews=("review_score", "count"),
        )
    else:
        agg_rev = pd.DataFrame(columns=["order_id", "review_score_mean", "review_score_median", "n_reviews"])

    # ---- Products + category translation ----
    prod = products.copy()
    if "category_translation" in dfs and "product_category_name" in prod.columns:
        cat = dfs["category_translation"].copy()
        # standardize column names for a clean join
        # typical columns: product_category_name, product_category_name_english or *_spanish/portuguese
        # We'll keep the original + a "_translated" best-effort column.
        trans_col = None
        # common candidates present in Kaggle translation file
        for cand in ["product_category_name_english", "product_category_name_english " , "product_category_name_english\n"]:
            if cand in cat.columns:
                trans_col = cand
                break
        if trans_col is None and len(cat.columns) >= 2:
            # fallback: pick the second column as translation
            trans_col = cat.columns[1]
        cat = cat.rename(columns={trans_col: "product_category_name_english"})
        prod = prod.merge(cat[["product_category_name", "product_category_name_english"]].drop_duplicates(),
                          on="product_category_name", how="left")

    # ---- Geolocation aggregated per zip_code_prefix ----
    geo_agg = None
    if "geolocation" in dfs and {"geolocation_zip_code_prefix", "geolocation_lat", "geolocation_lng"}.issubset(set(dfs["geolocation"].columns)):
        geo = dfs["geolocation"][["geolocation_zip_code_prefix", "geolocation_lat", "geolocation_lng"]].copy()
        geo["geolocation_lat"] = pd.to_numeric(geo["geolocation_lat"], errors="coerce")
        geo["geolocation_lng"] = pd.to_numeric(geo["geolocation_lng"], errors="coerce")
        geo_agg = geo.groupby("geolocation_zip_code_prefix", as_index=False).agg(
            lat=("geolocation_lat", "mean"),
            lng=("geolocation_lng", "mean"),
            n_points=("geolocation_lat", "count")
        )

    # ---- Build fact table: one row per order item ----
    fact = items.merge(orders, on="order_id", how="left", suffixes=("", "_order"))
    fact = fact.merge(prod, on="product_id", how="left", suffixes=("", "_product"))
    fact = fact.merge(sellers, on="seller_id", how="left", suffixes=("", "_seller"))
    fact = fact.merge(customers, on="customer_id", how="left", suffixes=("", "_customer"))
    fact = fact.merge(agg_total, on="order_id", how="left")
    fact = fact.merge(agg_rev, on="order_id", how="left")

    # attach geo for customer and seller if available
    if geo_agg is not None:
        if "customer_zip_code_prefix" in fact.columns:
            fact = fact.merge(
                geo_agg.rename(columns={
                    "geolocation_zip_code_prefix": "customer_zip_code_prefix",
                    "lat": "customer_lat",
                    "lng": "customer_lng",
                    "n_points": "customer_zip_points"
                }),
                on="customer_zip_code_prefix", how="left"
            )
        if "seller_zip_code_prefix" in fact.columns:
            fact = fact.merge(
                geo_agg.rename(columns={
                    "geolocation_zip_code_prefix": "seller_zip_code_prefix",
                    "lat": "seller_lat",
                    "lng": "seller_lng",
                    "n_points": "seller_zip_points"
                }),
                on="seller_zip_code_prefix", how="left"
            )

    # Numeric coercions for price/freight
    for c in ["price", "freight_value"]:
        if c in fact.columns:
            fact[c] = pd.to_numeric(fact[c], errors="coerce")

    # Derived KPIs
    if {"price", "freight_value"}.issubset(fact.columns):
        fact["item_gmv"] = fact["price"].fillna(0) + fact["freight_value"].fillna(0)

    # Arrange useful columns (if present)
    preferred_cols = [
        # Keys
        "order_id", "order_item_id", "product_id", "seller_id", "customer_id",
        # Order
        "order_status", "order_purchase_timestamp", "order_approved_at",
        "order_delivered_carrier_date", "order_delivered_customer_date",
        "order_estimated_delivery_date",
        # Item values
        "price", "freight_value", "item_gmv", "shipping_limit_date",
        # Customer dims
        "customer_city", "customer_state", "customer_zip_code_prefix",
        "customer_lat", "customer_lng",
        # Seller dims
        "seller_city", "seller_state", "seller_zip_code_prefix",
        "seller_lat", "seller_lng",
        # Product dims
        "product_category_name", "product_category_name_english", "product_weight_g",
        "product_length_cm", "product_height_cm", "product_width_cm",
        # Payments & reviews
        "total_payment_value", "n_payments", "review_score_mean", "n_reviews",
    ]
    cols_existing = [c for c in preferred_cols if c in fact.columns]
    fact = fact[cols_existing + [c for c in fact.columns if c not in cols_existing]]

    return fact

def save_outputs(fact: pd.DataFrame, output_dir: str) -> Dict[str, str]:
    """
    Save denormalized fact to CSV and Parquet (if available).
    Also save light-weight dimension extracts for convenience.
    Returns a dict with output file paths.
    """
    paths: Dict[str, str] = {}

    fact_csv = os.path.join(output_dir, "olist_order_lines_denorm.csv")
    fact.to_csv(fact_csv, index=False)
    paths["fact_csv"] = fact_csv

    try:
        fact_parquet = os.path.join(output_dir, "olist_order_lines_denorm.parquet")
        fact.to_parquet(fact_parquet, index=False)
        paths["fact_parquet"] = fact_parquet
    except Exception as e:
        print(f"[INFO] Skipping Parquet (pyarrow/fastparquet not installed?): {e}")

    # Dimension extracts
    if "product_category_name" in fact.columns:
        dim_prod = fact[["product_id", "product_category_name", "product_category_name_english"]]\
                       .drop_duplicates()
        p = os.path.join(output_dir, "dim_products.csv")
        dim_prod.to_csv(p, index=False)
        paths["dim_products"] = p

    if {"seller_id", "seller_city", "seller_state"}.issubset(fact.columns):
        dim_sell = fact[["seller_id", "seller_city", "seller_state"]].drop_duplicates()
        p = os.path.join(output_dir, "dim_sellers.csv")
        dim_sell.to_csv(p, index=False)
        paths["dim_sellers"] = p

    if {"customer_id", "customer_city", "customer_state"}.issubset(fact.columns):
        dim_cust = fact[["customer_id", "customer_city", "customer_state"]].drop_duplicates()
        p = os.path.join(output_dir, "dim_customers.csv")
        dim_cust.to_csv(p, index=False)
        paths["dim_customers"] = p

    return paths

def main(data_dir: Optional[str] = None) -> None:
    data_dir = data_dir or DATA_DIR
    print(f"[INFO] Reading input files from: {data_dir}")
    dfs = load_tables(data_dir)
    fact = build_denorm(dfs)
    print(f"[OK] Denormalized order lines shape: {fact.shape}")
    outputs = save_outputs(fact, OUTPUT_DIR)
    print("[OK] Files written:")
    for k, v in outputs.items():
        print(f"  - {k}: {v}")

if __name__ == "__main__":
    # Allow an optional custom data directory via CLI
    custom_dir = sys.argv[1] if len(sys.argv) > 1 else None
    main(custom_dir)
